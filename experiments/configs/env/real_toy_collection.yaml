# @package _global_
cross_validate: False
save_intermedia_models: True

dataset_generator_func:
  _target_: env.realworld.rollout_runner.convert_dataset_image
  # episode_num_pertask: 10000 
  # env_names: ["RealRobotDrakeHammerforhammerEnv-Tool"]
  task_name: "toy collection"
  dataset_dir: "/home/yunzhe/locoman_teleop/demonstrations/${domains}"
  action_name: ['actions/body', 'actions/eef', 'actions/gripper']
  observation_name: ['proprioceptions/body', 'proprioceptions/eef', 'proprioceptions/gripper', 'proprioceptions/eef_to_body']
  camera_names: ['main_left', 'main_right', 'wrist']

# test
rollout_runner:
  _target_: env.realworld.rollout_runner.RolloutRunner
  episode_num: 50
  env_names: ["DrakeHammerEnv"]
  save_video: True

dataloader:
  batch_size: 32
  num_workers: 1
  pin_memory: True
  persistent_workers: True
  shuffle: True
  drop_last: False

val_dataloader:
  batch_size: 32
  num_workers: 1
  shuffle: False
  pin_memory: True
  persistent_workers: True
  drop_last: False

dataset:
  _target_: hpt.dataset.local_traj_dataset.LocalTrajDataset
  horizon: 1 # horizon for each dataset sample. not used
  val_ratio: 0.03 # the train-validation ratio
  pad_after: 59 # padding after the episode
  episode_cnt: 20 # total episodes by default
  step_cnt: 100000 # total data transitions
  data_augmentation: False  # data augmentation
  use_disk: False # use disk instead of memory to store the data
  pad_before: 59 # padding before the episode
  data_ratio: 1 # only use a fraction of data
  action_horizon: 60 # observation: (observation + action) is action horizon
  observation_horizon: 1 # before observation horizon is observation
  dataset_postfix: "_traj${dataset.episode_cnt}" # postfix for the dataset
  precompute_feat: True # precompute features using pretrained models for stems
  image_encoder: 'resnet' # which encoder to use as the pretrained model
  dataset_encoder_postfix: "_${dataset.image_encoder}" # another postfix
  use_multiview: False # use multiple camera views
  normalize_state: True # whether to normalize the states in datasets
  regenerate: False # regenerate data
  action_multiple_horizon: False  # multiply action dimensions by horizon
  random_mask_obs: True # whether to randomize observation input length
  data_augment_ratio: 1 # add data augmentation to the images
  proprioception_expand: False # expand proprioception to use multiple tokens
  proprioception_expand_dim: 32 # expand proprioception dimensions
  task_description: "toy collection" # task description, overrides the task_name in dataset_generator_func when computing T5 features

head:
  _target_: hpt.models.policy_head.TransformerDecoder
  input_dim: ${network.embed_dim}
  output_dim: -1 # overwrite based on dataset
  action_horizon: ${dataset.action_horizon} # action horizon
  crossattn_modality_dropout: 0.1
  crossattn_heads: 8
  crossattn_dim_head: 64

train:
  total_epochs: 500  # maximum training epochs before termination. usually set as maximum
  total_iters: 60000 # maximum training steps before termination
  epoch_iters: 1000  # training steps in each epoch
  validation_iters: 100 # maximum iterations for validation
  pretrained_dir: "" # pretrained model path for testing
  freeze_trunk: True # whether to freeze the trunk during finetuning
  wandb_pretrained_dir: "" # use models pretrained on wandb


domains: real_toy_collection